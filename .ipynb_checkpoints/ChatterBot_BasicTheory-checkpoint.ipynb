{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#深入语言模型\" data-toc-modified-id=\"深入语言模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>深入语言模型</a></div><div class=\"lev2 toc-item\"><a href=\"#理解语言模型\" data-toc-modified-id=\"理解语言模型-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>理解语言模型</a></div><div class=\"lev2 toc-item\"><a href=\"#语言模型的困难\" data-toc-modified-id=\"语言模型的困难-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>语言模型的困难</a></div><div class=\"lev1 toc-item\"><a href=\"#中文分词艺术\" data-toc-modified-id=\"中文分词艺术-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>中文分词艺术</a></div><div class=\"lev2 toc-item\"><a href=\"#n-元语法模型\" data-toc-modified-id=\"n-元语法模型-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>n 元语法模型</a></div><div class=\"lev2 toc-item\"><a href=\"#基于字构词\" data-toc-modified-id=\"基于字构词-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>基于字构词</a></div><div class=\"lev1 toc-item\"><a href=\"#概率图模型\" data-toc-modified-id=\"概率图模型-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>概率图模型</a></div><div class=\"lev2 toc-item\"><a href=\"#图论\" data-toc-modified-id=\"图论-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>图论</a></div><div class=\"lev2 toc-item\"><a href=\"#概率图模型\" data-toc-modified-id=\"概率图模型-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>概率图模型</a></div><div class=\"lev2 toc-item\"><a href=\"#贝叶斯网络\" data-toc-modified-id=\"贝叶斯网络-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>贝叶斯网络</a></div><div class=\"lev2 toc-item\"><a href=\"#马尔可夫与隐马尔可夫模型\" data-toc-modified-id=\"马尔可夫与隐马尔可夫模型-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>马尔可夫与隐马尔可夫模型</a></div><div class=\"lev2 toc-item\"><a href=\"#最大熵模型\" data-toc-modified-id=\"最大熵模型-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>最大熵模型</a></div><div class=\"lev2 toc-item\"><a href=\"#条件随机场\" data-toc-modified-id=\"条件随机场-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>条件随机场</a></div><div class=\"lev1 toc-item\"><a href=\"#命名实体\" data-toc-modified-id=\"命名实体-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>命名实体</a></div><div class=\"lev1 toc-item\"><a href=\"#词性标注\" data-toc-modified-id=\"词性标注-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>词性标注</a></div><div class=\"lev2 toc-item\"><a href=\"#词性标注过程\" data-toc-modified-id=\"词性标注过程-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>词性标注过程</a></div><div class=\"lev2 toc-item\"><a href=\"#词性标注的具体方法\" data-toc-modified-id=\"词性标注的具体方法-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>词性标注的具体方法</a></div><div class=\"lev2 toc-item\"><a href=\"#词性标注的校验\" data-toc-modified-id=\"词性标注的校验-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>词性标注的校验</a></div><div class=\"lev1 toc-item\"><a href=\"#句法分析\" data-toc-modified-id=\"句法分析-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>句法分析</a></div><div class=\"lev2 toc-item\"><a href=\"#句法结构分析基本方法\" data-toc-modified-id=\"句法结构分析基本方法-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>句法结构分析基本方法</a></div><div class=\"lev2 toc-item\"><a href=\"#计算过程\" data-toc-modified-id=\"计算过程-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>计算过程</a></div><div class=\"lev2 toc-item\"><a href=\"#句法规则提取方法与PCFG的概率参数估计\" data-toc-modified-id=\"句法规则提取方法与PCFG的概率参数估计-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>句法规则提取方法与PCFG的概率参数估计</a></div><div class=\"lev2 toc-item\"><a href=\"#总结\" data-toc-modified-id=\"总结-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>总结</a></div><div class=\"lev1 toc-item\"><a href=\"#词义消歧\" data-toc-modified-id=\"词义消歧-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>词义消歧</a></div><div class=\"lev2 toc-item\"><a href=\"#有监督的词义消歧方法\" data-toc-modified-id=\"有监督的词义消歧方法-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>有监督的词义消歧方法</a></div><div class=\"lev3 toc-item\"><a href=\"#基于互信息的词义消歧方法\" data-toc-modified-id=\"基于互信息的词义消歧方法-711\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>基于互信息的词义消歧方法</a></div><div class=\"lev3 toc-item\"><a href=\"#基于贝叶斯分类器的消歧方法\" data-toc-modified-id=\"基于贝叶斯分类器的消歧方法-712\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>基于贝叶斯分类器的消歧方法</a></div><div class=\"lev2 toc-item\"><a href=\"#无监督的词义消歧方法\" data-toc-modified-id=\"无监督的词义消歧方法-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>无监督的词义消歧方法</a></div><div class=\"lev1 toc-item\"><a href=\"#语义角色标注\" data-toc-modified-id=\"语义角色标注-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>语义角色标注</a></div><div class=\"lev2 toc-item\"><a href=\"#基于短语结构树的语义角色标注方法\" data-toc-modified-id=\"基于短语结构树的语义角色标注方法-81\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>基于短语结构树的语义角色标注方法</a></div><div class=\"lev2 toc-item\"><a href=\"#基于依存句法分析结果和基于语块的语义角色标注方法\" data-toc-modified-id=\"基于依存句法分析结果和基于语块的语义角色标注方法-82\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>基于依存句法分析结果和基于语块的语义角色标注方法</a></div><div class=\"lev1 toc-item\"><a href=\"#隐含语义索引模型\" data-toc-modified-id=\"隐含语义索引模型-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>隐含语义索引模型</a></div><div class=\"lev2 toc-item\"><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>TF-IDF</a></div><div class=\"lev2 toc-item\"><a href=\"#隐含语义索引模型\" data-toc-modified-id=\"隐含语义索引模型-92\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>隐含语义索引模型</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深入语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**什么是数学建模**  \n",
    "数学建模就是通过计算得到的结果来解释实际问题，并接受实际的检验，来建立数学模型的全过程。\n",
    "\n",
    " \n",
    "\n",
    "**什么是语言模型**  \n",
    "语言模型是根据语言客观事实而进行的语言抽象数学建模。说白了，就是找到一个数学模型，让它来解释自然语言的事实。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理解语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**业界认可的语言模型**  \n",
    "业界目前比较认可而且有效的语言模型是n元语法模型(n-gram model)，它本质上是马尔可夫模型，简单来描述就是：一句话中下一个词的出现和最近n个词有关(包括它自身)。详细解释一下：\n",
    "\n",
    "如果这里的n=1时，那么最新一个词只和它自己有关，也就是它是独立的，和前面的词没关系，这叫做一元文法\n",
    "\n",
    "如果这里的n=2时，那么最新一个词和它前面一个词有关，比如前面的词是“我”，那么最新的这个词是“是”的概率比较高，这叫做二元文法，也叫作一阶马尔科夫链\n",
    "\n",
    "依次类推，工程上n=3用的是最多的，因为n越大约束信息越多，n越小可靠性更高\n",
    "\n",
    "n元语法模型实际上是一个概率模型，也就是出现一个词的概率是多少，或者一个句子长这个样子的概率是多少。\n",
    "\n",
    "这就又回到了之前文章里提到的自然语言处理研究的两大方向：基于规则、基于统计。n元语法模型显然是基于统计的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**概率是如何统计的**  \n",
    "说到基于统计，那么就要说概率是如何估计的了，通常都是使用最大似然估计，怎么样理解“最大似然估计”，最大似然就是最最最最最相似的，那么和谁相似，和历史相似，历史是什么样的？10个词里出现过2次，所以是2/10=1/5，所以经常听说过的“最大似然估计”就是用历史出现的频率来估计概率的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型的困难"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 千变万化的自然语言导致的0概率问题**  \n",
    "基于统计的自然语言处理需要基于大量语料库进行，而自然语言千变万化，可以理解所有词汇的笛卡尔积，数量大到无法想象，有限的语料库是难以穷举语言现象的，因此n元语法模型会出现某一句话出现的概率为0的情况，比如我这篇博客在我写出来之前概率就是0，因为我是原创。那么这个0概率的问题如何解决呢？这就是业界不断在研究的数据平滑技术，也就是通过各种数学方式来让每一句话的概率都大于0。具体方法不列举，都是玩数学的，比较简单，无非就是加个数或者减个数或者做个插值平滑一下，效果上应用在不同特点的数据上各有千秋。平滑的方法确实有效，各种自然语言工具中都实现了，直接用就好了。\n",
    "\n",
    " \n",
    "\n",
    "**2. 特定领域的特定词概率偏大问题**  \n",
    "每一种领域都会有一些词汇比正常概率偏大，比如计算机领域会经常出现“性能”、“程序”等词汇，这个解决办法可以通过缓存一些刚刚出现过的词汇来提高后面出现的概率来解决。当然这里面是有很多技巧的，我们并不是认为所有出现过的词后面概率都较大，而是会考虑这些词出现的频率和规律(如：词距)来预测。\n",
    "\n",
    " \n",
    "\n",
    "**3. 单一语言模型总会有弊端**  \n",
    "还是因为语料库的不足，我们会融合多种语料库，但因为不同语料库之间的差异，导致我们用单一语言模型往往不够准确，因此，有一种方法可以缓和这种不准确性，那就是把多种语言模型混到一起来计算，这其实是一种折中，这种方法low且有效。\n",
    "\n",
    "还有一种方法就是用多种语言模型来分别计算，最后选择熵最大的一种，这其实也是一种折中，用在哪种地方就让哪种模型生效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**神经网络语言模型**  \n",
    "21世纪以来，统计学习领域无论什么都要和深度学习搭个边，毕竟计算机计算能力提升了很多，无论多深都不怕。神经网络语言模型可以看做是一种特殊的模型平滑方式，本质上还是在计算概率，只不过通过深层的学习来得到更正确的概率。\n",
    "\n",
    " \n",
    "**语言模型的应用**  \n",
    "这几乎就是自然语言处理的应用了，有：中文分词、机器翻译、拼写纠错、语音识别、音子转换、自动文摘、问答系统、OCR等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词艺术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**中文分词是怎么走到今天的**  \n",
    "\n",
    "话说上个世纪，中文自动分词还处于初级阶段，每句话都要到汉语词表中查找，有没有这个词？有没有这个词？所以研究集中在：怎么查找最快、最全、最准、最狠......，所以就出现了正向最大匹配法、逆向最大匹配法、双向扫描法、助词遍历法......，用新世纪比较流行的一个词来形容就是：你太low了！\n",
    "\n",
    "中文自动分词最难的两个问题：1）歧义消除；2）未登陆词识别。说句公道话，没有上个世纪那么low的奠定基础，也就没有这个世纪研究重点提升到这两个高级的问题\n",
    "\n",
    "ps:未登录词就是新词，词表里没有的词\n",
    "\n",
    "本世纪计算机软硬件发展迅猛，计算量存储量都不再是问题，因此基于统计学习的自动分词技术成为主流，所以就出现了各种新分词方法，也更适用于新世纪文本特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n 元语法模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从n元语法模型开始说起**  \n",
    "上节讲到了n元语法模型，在前n-1个词出现的条件下，下一个词出现的概率是有统计规律的，这个规律为中文自动分词提供了统计学基础，所以出现了这么几种统计分词方法：N-最短路径分词法、基于n元语法模型的分词法\n",
    "\n",
    "N-最短路径分词法其实就是一元语法模型，每个词成为一元，独立存在，出现的概率可以基于大量语料统计得出，比如“确实”这个词出现概率的0.001（当然这是假设，别当真），我们把一句话基于词表的各种切词结果都列出来，因为字字组合可能有很多种，所以有多个候选结果，这时我们利用每个词出现的概率相乘起来，得到的最终结果，谁最大谁就最有可能是正确的，这就是N-最短路径分词法。\n",
    "\n",
    "这里的N的意思是说我们计算概率的时候最多只考虑前N个词，因为一个句子可能很长很长，词离得远，相关性就没有那么强了\n",
    "\n",
    "这里的最短路径其实是传统最短路径的一种延伸，由加权延伸到了概率乘积\n",
    "\n",
    "而基于n元语法模型的分词法就是在N-最短路径分词法基础上把一元模型扩展成n元模型，也就是统计出的概率不再是一个词的概率，而是基于前面n个词的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于字构词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由字构词的分词方法出现可以说是一项突破，发明者也因此得到了各项第一和很多奖项，那么这个著名的分词法是怎么做的呢？\n",
    "\n",
    "每个字在词语中都有一个构词位置：词首、词中、词尾、单独构词。根据一个字属于不同的构词位置，我们设计出来一系列特征，比如：前一个词、前两个词、前面词长度、前面词词首、前面词词尾、前面词词尾加上当前的字组成的词……\n",
    "\n",
    "我们基于大量语料库，利用平均感知机分类器对上面特征做打分，并训练权重系数，这样得出的模型就可以用来分词了，句子右边多出来一个字，用模型计算这些特征的加权得分，得分最高的就是正确的分词方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**流行分词工具都是用的什么分词方法**  \n",
    "- jieba中文分词  \n",
    "官方描述：  \n",
    "基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)\n",
    "采用了动态规划查找最大概率路径, 找出基于词频的最大切分 组合\n",
    "对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法\n",
    "前两句话是说它是基于词表的分词，最后一句是说它也用了由字构词，所以它结合了两种分词方法\n",
    "\n",
    " \n",
    "\n",
    "- ik分词器\n",
    "基于词表的最短路径切词\n",
    "\n",
    " \n",
    "\n",
    "- ltp云平台分词\n",
    "主要基于机器学习框架并部分结合词表的方法\n",
    "\n",
    " \n",
    "\n",
    "其他分词工具判断方法类似，网上对各种分词工具好坏的判断多数是功能上比较，个人建议通过原理来判断，如果结合了基于词表和由字构词并且充分利用统计学习的方法，这样的分词工具才是最好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率图模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图就是把一些孤立的点用线连起来，任何点之间都有可能连着。它区别于树，树是有父子关系，图没有。  \n",
    "\n",
    "从质上来说，图可以表达的某些事物之间的关联关系，也可以表达的是一种转化关系；从量上来说，它能表达出关联程度，也能表达出转化的可能性大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率图模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做一个分类，分成有向图模型和无向图模型，顾名思义，就是图里面的边是否有方向。  \n",
    "\n",
    "那么什么样的模型的边有方向，而什么样的没方向呢？这个很好想到，有方向的表达的是一种推演关系，也就是在A的前提下出现了B，这种模型又叫做生成式模型。而没有方向表达的是一种“这样就对了”的关系，也就是A和B同时存在就对了，这种模型又叫做判别式模型。  \n",
    "\n",
    "生成式模型一般用联合概率计算(因为我们知道A的前提了，可以算联合概率)，判别式模型一般用条件概率计算(因为我们不知道前提，所以只能\"假设\"A条件下B的概率)。  \n",
    "\n",
    "生成式模型的代表是：n元语法模型、隐马尔可夫模型、朴素贝叶斯模型等。判别式模型的代表是：最大熵模型、支持向量机、条件随机场、感知机模型等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯网络模型可以通过样本学习来估计每个节点的概率，从而达到可以预测各种问题的结果\n",
    "\n",
    "贝叶斯网络能够在已知有限的、不完整的、不确定信息条件下进行学习推理，所以广泛应用在故障诊断、维修决策、汉语自动分词、词义消歧等问题上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 马尔可夫与隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐马尔可夫模型，这里的“隐”指的是其中某一阶的信息我们不知道，就像是我们知道人的祖先是三叶虫，但是由三叶虫经历了怎样的演变过程才演变到人的样子我们是不知道的，我们只能通过化石资料了解分布信息，如果这类资料很多，那么就可以利用隐马尔可夫模型来建模，因为缺少的信息较多，所以这一模型的算法比较复杂，比如前向算法、后向算法之类晦涩的东西就不说了。相对于原理，我们更关注它的应用，隐马尔可夫模型广泛应用在词性标注、中文分词等，为什么能用在这两个应用上呢？仔细想一下能看得出来，比如中文分词，最初你是不知道怎么分词的，前面的词分出来了，你才之后后面的边界在哪里，但是当你后面做了分词之后还要验证前面的分词是否正确，这样前后有依赖关系，而不确定中间状态的情况最适合用隐马尔可夫模型来解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大熵模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么理解最大熵模型呢？我们的最终目的是想知道在某一个信息条件B下，得出某种可能的结果A的最大的概率，也就是条件概率P(A|B)最大的候选结果。因为最大熵就是不确定性最大，其实也就是条件概率最大，所以求最大的条件概率等同于求最大熵，而我们这里的熵其实是H(p)=H(A|B)=-∑p(b)p(a|b)log(p(a|b))，为了使用训练数据做估计，这里的p(a|b)可以通过训练数据的某些特征来估计，比如这些特征是fi(a,b)，那么做模型训练的过程就编程了训练∑λf(a,b)中的λ参数的过程，至此就有些像机器学习的线性回归了，该怎么做就清晰了。所以其实最大熵模型就是利用熵的原理和熵的公式来用另外一种形式来描述具有概率规律的实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件随机场\n",
    "\n",
    "\n",
    "场表示取值范围，随机场表示随机变量有取值范围，也就是每个随机变量有固定的取值，条件指的是随机变量的取值由一定的条件概率决定，而这里的条件来自于我们有一些观察值，这是它区别于其他随机场的地方。条件随机场也可以看做是一个无向图模型，它特殊就特殊在给定观察序列X时某个特定的标记序列Y的概率是一个指数函数exp(∑λt+∑μs)，其中t是转移函数，s是状态函数，我们需要训练的是λ和μ。条件随机场主要应用在标注和切分有序数据上，尤其在自然语言处理、生物信息学、机器视觉、网络智能等方面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下，概率图模型包括多种结合概率论和图论的模型，根据特定场景特定需求选择不同的模型，每种模型的参数都需要大量样本训练得出，每种模型都是用来根据训练出来的概率做最优结论选择的，比如根据训练出来的模型对句子做最正确的词性标注、实体标注、分词序列等，本文只是从理念上的解释和总结，真的用到某一种模型还是需要深入研究原理和公式推导以及编程实现，那就不是本文这种小篇幅能够解释的完的了，等我们后面要遇到必须用某一种模型来实现时再狠狠地深入一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 命名实体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概率图模型中的条件随机场适用于在一定观测值条件下决定的随机变量有有限个取值的情况，它特殊就特殊在给定观察序列X时某个特定的标记序列Y的概率是一个指数函数exp(∑λt+∑μs)，这也正符合最大熵原理。基于条件随机场的命名实体识别方法属于有监督的学习方法，需要利用已经标注好的大规模语料库进行训练，那么已经标注好的语料里面有什么样的特征能够让模型得以学习呢？\n",
    "\n",
    "我们采用当前位置的前后n个位置上的字/词/字母/数字/标点等作为特征，因为是基于已经标注好的语料，所以这些特征是什么样的词性、词形都是已知的。\n",
    "\n",
    "特征模板的选择是和具体我们要识别的实体类别有关系的，识别人名和识别机构名用的特征模板是不一样的，因为他们的特点就不一样，事实上识别中文人名和识别英文人名用的特征模板也是不一样的，因为他们的特点就不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常说的词性包括：名、动、形、数、量、代、副、介、连、助、叹、拟声。但自然语言处理中要分辨的词性要更多更精细，比如：区别词、方位词、成语、习用语、机构团体、时间词等，多达100多种。\n",
    "\n",
    "汉语词性标注最大的困难是“兼类”，也就是一个词在不同语境中有不同的词性，而且很难从形式上识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注过程\n",
    "\n",
    "为了解决词性标注无法达到100%准确的问题，词性标注一般要经过“标注”和“校验”两个过程，第一步“标注”根据规则或统计的方法做词性标注，第二步“校验”通过一致性检查和自动校对等方法来修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注的具体方法\n",
    "\n",
    "\n",
    "词性标注具体方法包括：基于统计模型的方法、基于规则的方法和两者结合的方法。下面我们分别来介绍。\n",
    "\n",
    " \n",
    "\n",
    "**基于统计模型的词性标注方法**  \n",
    "提到基于统计模型，势必意味着我们要利用大量已经标注好的语料库来做训练，同时要先选择一个合适的训练用的数学模型，《自己动手做聊天机器人 十五-一篇文章读懂拿了图灵奖和诺贝尔奖的概率图模型》中我们介绍了概率图模型中的隐马尔科夫模型(HMM)比较适合词性标注这种基于观察序列来做标注的情形。语言模型选择好了，下面要做的就是基于语料库来训练模型参数，那么我们模型参数初值如何设置呢？这里面就有技巧了\n",
    "\n",
    " \n",
    "\n",
    "**隐马尔可夫模型参数初始化的技巧**  \n",
    "模型参数初始化是在我们尚未利用语料库之前用最小的成本和最接近最优解的目标来设定初值。HMM是一种基于条件概率的生成式模型，所以模型参数是生成概率，那么我们不妨就假设每个词的生成概率就是它所有可能的词性个数的倒数，这个是计算最简单又最有可能接近最优解的生成概率了。每个词的所有可能的词性是我们已经有的词表里标记好的，这个词表的生成方法就比较简单了，我们不是有已经标注好的语料库嘛，很好统计。那么如果某个词在词表里没有呢？这时我们可以把它的生成概率初值设置为0。这就是隐马尔可夫模型参数初始化的技巧，总之原则就是用最小的成本和最接近最优解的目标来设定初值。一旦完成初始值设定后就可以利用前向后向算法进行训练了。\n",
    "\n",
    "\n",
    "**基于规则的词性标注方法**  \n",
    "规则就是我们既定好一批搭配关系和上下文语境的规则，判断实际语境符合哪一种则按照规则来标注词性。这种方法比较古老，适合于既有规则，对于兼词的词性识别效果较好，但不适合于如今网络新词层出不穷、网络用语新规则的情况。于是乎，有人开始研究通过机器学习来自动提取规则，怎么提取呢？不是随便给一堆语料，它直接来生成规则，而是根据初始标注器标注出来的结果和人工标注的结果的差距，来生成一种修正标注的转换规则，这是一种错误驱动的学习方法。基于规则的方法还有一个好处在于：经过人工校总结出的大量有用信息可以补充和调整规则库，这是统计方法做不到的。\n",
    "\n",
    " \n",
    "\n",
    "**统计方法和规则方法相结合的词性标注方法**  \n",
    "统计方法覆盖面比较广，新词老词通吃，常规非常规通吃，但对兼词、歧义等总是用经验判断，效果不好。规则方法对兼词、歧义识别比较擅长，但是规则总是覆盖不全。因此两者结合再好不过，先通过规则排歧，再通过统计标注，最后经过校对，可以得到正确的标注结果。在两者结合的词性标注方法中，有一种思路可以充分发挥两者优势，避免劣势，就是首选统计方法标注，同时计算计算它的置信度或错误率，这样来判断是否结果是否可疑，在可疑情况下采用规则方法来进行歧义消解，这样达到最佳效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注的校验\n",
    "\n",
    "\n",
    "做完词性标注并没有结束，需要经过校验来确定正确性以及修正结果。\n",
    "\n",
    "第一种校验方法就是检查词性标注的一致性。一致性指的是在所有标注的结果中，具有相同语境下同一个词的标注是否都相同，那么是什么原因导致的这种不一致呢？一种情况就是这类词就是兼类词，可能被标记为不同词性。另一种情况是非兼类词，但是由于人工校验或者其他原因导致标记为不同词性。达到100%的一致性是不可能的，所以我们需要保证一致性处于某个范围内，由于词数目较多，词性较多，一致性指标无法通过某一种计算公式来求得，因此可以基于聚类和分类的方法，根据欧式距离来定义一致性指标，并设定一个阈值，保证一致性在阈值范围内。\n",
    "\n",
    "第二种校验方法就是词性标注的自动校对。自动校对顾名思义就是不需要人参与，直接找出错误的标注并修正，这种方法更适用于一个词的词性标注通篇全错的情况，因为这种情况基于数据挖掘和规则学习方法来做判断会相对比较准确。通过大规模训练语料来生成词性校对决策表，然后根据这个决策表来找通篇全错的词性标注并做自动修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 句法分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句法分析分为句法结构分析和依存关系分析。\n",
    "\n",
    "句法结构分析也就是短语结构分析，比如提取出句子中的名次短语、动词短语等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句法结构分析基本方法  \n",
    "\n",
    "分为基于规则的分析方法和基于统计的分析方法。基于规则的方法存在很多局限性，所以我们采取基于统计的方法，目前最成功的是基于概率上下文无关文法(PCFG)。基于PCFG分析需要有如下几个要素：终结符集合、非终结符集合、规则集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算过程 \n",
    "\n",
    "设W={ω1ω2ω3……}表示一个句子，其中的ω表示一个词(word)，利用动态规划算法计算非终结符A推导出W中子串ωiωi+1ωi+2……ωj的概率，  \n",
    "假设概率为αij(A)，那么有如下递归公式：\n",
    "\n",
    "αij(A)=P(A->ωi)  \n",
    "\n",
    "αij(A)=∑∑P(A->BC)αik(B)α(k+1)j(C)  \n",
    "\n",
    "以上两个式子好好理解一下其实就是上面“我吃肉”的计算过程\n",
    "\n",
    " \n",
    "\n",
    "以上过程理解了之后你一定会问，这里面最关键的的非终结符、终结符以及规则集是怎么得来的，概率又是怎么确定的？下面我们就来说明\n",
    "\n",
    " \n",
    "\n",
    "## 句法规则提取方法与PCFG的概率参数估计\n",
    "\n",
    "这部分就是机器学习的知识了，有关机器学习可以参考《机器学习教程》\n",
    "\n",
    "首先我们需要大量的树库，也就是训练数据。然后我们把树库中的句法规则提取出来生成我们想要的结构形式，并进行合并、归纳等处理，最终得到上面∑、N、R的样子。其中的概率参数计算方法是这样的：\n",
    "\n",
    "先给定参数为一个随机初始值，然后采用EM迭代算法，不断训练数据，并计算每条规则使用次数作为最大似然计算得到概率的估值，这样不断迭代更新概率，最终得出的概率可以认为是符合最大似然估计的精确值。\n",
    "\n",
    "∑={我, 吃, 肉,……}  \n",
    "\n",
    "N={S, VP, ……}  \n",
    "\n",
    "R={  \n",
    "NN->我     0.5  \n",
    "Vt->吃     1.0  \n",
    "NN->肉     0.5  \n",
    "VP->Vt NN  1.0  \n",
    "S->NN VP   1.0  \n",
    "……\n",
    "}\n",
    "\n",
    "\n",
    "## 总结\n",
    "句法分析树生成算法是基于统计学习的原理，根据大量标注的语料库（树库），通过机器学习算法得出非终结符、终结符、规则集及其概率参数，然后利用动态规划算法生成每一句话的句法分析树，在句法分析树生成过程中如果遇到多种树结构，选择概率最大的那一种作为最佳句子结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词义消歧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词义消歧是句子和篇章语义理解的基础，是必须解决的问题。  \n",
    "词义消歧可以通过机器学习的方法来解决。谈到机器学习就会分成有监督和无监督的机器学习。词义消歧有监督的机器学习方法也就是分类算法，即判断词义所属的分类。词义消歧无监督的机器学习方法也就是聚类算法，把词义聚成多类，每一类是一种含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有监督的词义消歧方法\n",
    "\n",
    "### 基于互信息的词义消歧方法\n",
    "\n",
    "这个方法的名字不好理解，但是原理却非常简单：用两种语言对照着看，比如：中文“打人”对应英文“beat a man”，而中文“打酱油”对应英文“buy some sauce”。这样就知道当上下文语境里有“人”的时候“打”的含义是beat，当上下文语境里有“酱油”的时候“打”的含义是buy。按照这种思路，基于大量中英文对照的语料库训练出来的模型就可以用来做词义消歧了，这种方法就叫做基于“互信息”的词义消歧方法。讲到“互信息”还要说一下它的起源，它来源于信息论，表达的是一个随机变量中包含另一个随机变量的信息量(也就是英文信息中包含中文信息的信息量)，假设两个随机变量X、Y的概率分别是p(x), p(y)，它们的联合分布概率是p(x,y)，那么互信息计算公式是：  \n",
    "\n",
    "$I(X; Y) = ∑∑p(x,y)log(p(x,y)/(p(x)p(y)))$  \n",
    "\n",
    "以上公式是怎么推导出来的呢？比较简单，“互信息”可以理解为一个随机变量由于已知另一个随机变量而减少的不确定性(也就是理解中文时由于已知了英文的含义而让中文理解更确定了)，因为“不确定性”就是熵所表达的含义，所以：  \n",
    "\n",
    "$I(X; Y) = H(X) - H(X|Y)$  \n",
    "\n",
    "等式后面经过不断推导就可以得出上面的公式。  \n",
    "那么我们在对语料不断迭代训练过程中I(X; Y)是不断减小的，算法终止的条件就是I(X; Y)不再减小。  \n",
    "基于互信息的词义消歧方法自然对机器翻译系统的效果是最好的，但它的缺点是：双语语料有限，多种语言能识别出歧义的情况也是有限的(比如中英文同一个词都有歧义就不行了)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于贝叶斯分类器的消歧方法\n",
    "提到贝叶斯那么一定少不了条件概率，这里的条件指的就是上下文语境这个条件，任何多义词的含义都是跟上下文语境相关的。假设语境(context)记作c，语义(semantic)记作s，多义词(word)记作w，那么我要计算的就是多义词w在语境c下具有语义s的概率，即：  \n",
    "\n",
    "$p(s|c)$  \n",
    "\n",
    "那么根据贝叶斯公式：\n",
    "\n",
    "$p(s|c) = p(c|s)p(s)/p(c)$  \n",
    "\n",
    "我要计算的就是p(s|c)中s取某一个语义的最大概率，因为p(c)是既定的，所以只考虑分子的最大值：\n",
    "\n",
    "$s的估计=max(p(c|s)p(s))$  \n",
    "\n",
    "因为语境c在自然语言处理中必须通过词来表达，也就是由多个v(词)组成，那么也就是计算：\n",
    "\n",
    "$max(p(s)∏p(v|s))$\n",
    "\n",
    "下面就是训练的过程了：\n",
    "\n",
    "p(s)表达的是多义词w的某个语义s的概率，可以统计大量语料通过最大似然估计求得：\n",
    "\n",
    "$p(s) = N(s)/N(w)$  \n",
    "\n",
    "p(v|s)表达的是多义词w的某个语义s的条件下出现词v的概率，可以统计大量语料通过最大似然估计求得：\n",
    "\n",
    "$p(v|s) = N(v, s)/N(s)$  \n",
    "\n",
    "训练出p(s)和p(v|s)之后我们对一个多义词w消歧的过程就是计算(p(c|s)p(s))的最大概率的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督的词义消歧方法\n",
    "\n",
    "完全无监督的词义消歧是不可能的，因为没有标注是无法定义是什么词义的，但是可以通过无监督的方法来做词义辨识。无监督的词义辨识其实也是一种贝叶斯分类器，和上面讲到的贝叶斯分类器消歧方法不同在于：这里的参数估计不是基于有标注的训练预料，而是先随机初始化参数p(v|s)，然后根据EM算法重新估计这个概率值，也就是对w的每一个上下文c计算p(c|s)，这样可以得到真实数据的似然值，回过来再重新估计p(v|s)，重新计算似然值，这样不断迭代不断更新模型参数，最终得到分类模型，可以对词进行分类，那么有歧义的词在不同语境中会被分到不同的类别里。\n",
    "\n",
    "仔细思考一下这种方法，其实是基于单语言的上下文向量的，那么我们进一步思考下一话题，如果一个新的语境没有训练模型中一样的向量怎么来识别语义？\n",
    "\n",
    "这里就涉及到向量相似性的概念了，我们可以通过计算两个向量之间夹角余弦值来比较相似性，即：\n",
    "\n",
    "$cos(a,b) = ∑ab/sqrt(∑a^2∑b^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语义角色标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "浅层语义标注是行之有效的语言分析方法，基于语义角色的浅层分析方法可以描述句子中语义角色之间的关系，是语义分析的重要方法，也是篇章分析的基础，本节介绍基于机器学习的语义角色标注方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 语义角色标注的基本方法**\n",
    "\n",
    "语义角色标注需要依赖句法分析的结果进行，因为句法分析包括短语结构分析、浅层句法分析、依存关系分析，  \n",
    "所以语义角色标注也分为：基于短语结构树的语义角色标注方法、基于浅层句法分析结果的语义角色标注方法、基于依存句法分析结果的语义角色标注方法。但无论哪种方法，过程都是：\n",
    "\n",
    "句法分析->候选论元剪除->论元识别->论元标注->语义角色标注结果\n",
    "\n",
    "- 其中论元剪除就是在较多候选项中去掉肯定不是论元的部分\n",
    "\n",
    "- 其中论元识别是一个二值分类问题，即：是论元和不是论元\n",
    "\n",
    "- 其中论元标注是一个多值分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于短语结构树的语义角色标注方法\n",
    "\n",
    "短语结构树是这样的结构：\n",
    "```\n",
    "S——|\n",
    "|        |\n",
    "NN    VP\n",
    "我      |——|\n",
    "          Vt     NN\n",
    "          吃     肉\n",
    "```\n",
    "短语结构树里面已经表达了一种结构关系，因此语义角色标注的过程就是依赖于这个结构关系来设计的一种复杂策略，策略的内容随着语言结构的复杂而复杂化，因此我们举几个简单的策略来说明。\n",
    "\n",
    "首先我们分析论元剪除的策略：\n",
    "\n",
    "因为语义角色是以谓词为中心的，因此在短语结构树中我们也以谓词所在的节点为中心，先平行分析，比如这里的“吃”是谓词，和他并列的是“肉”，明显“肉”是受事者，那么设计什么样的策略能使得它成为候选论元呢？我们知道如果“肉”存在一个短语结构的话，那么一定会多处一个树分支，那么“肉”和“吃”一定不会在树的同一层，**因此我们设计这样的策略来保证“肉”被选为候选论元：如果当前节点的兄弟节点和当前节点不是句法结构的并列关系，那么将它作为候选论元。**当然还有其他策略不需要记得很清楚，现用现查就行了，但它的精髓就是基于短语结构树的结构特点来设计策略的。\n",
    "\n",
    "然后就是论元识别过程了。论元识别是一个二值分类问题，因此一定是基于标注的语料库做机器学习的，机器学习的二值分类方法都是固定的，唯一的区别就是特征的设计，这里面一般设计如下特征效果比较好：谓词本身、短语结构树路径、短语类型、论元在谓词的位置、谓词语态、论元中心词、从属类别、论元第一个词和最后一个词、组合特征。\n",
    "\n",
    "论元识别之后就是论元标注过程了。这又是一个利用机器学习的多值分类器进行的，具体方法不再赘述。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于依存句法分析结果和基于语块的语义角色标注方法\n",
    "\n",
    "这两种语义角色标注方法和基于短语结构树的语义角色标注方法的主要区别在于论元剪除的过程，原因就是他们基于的句法结构不同。\n",
    "\n",
    "基于依存句法分析结果的语义角色标注方法会基于依存句法直接提取出谓词-论元关系，这和依存关系的表述是很接近的，因此剪除策略的设计也就比较简单：以谓词作为当前节点，当前节点所有子节点都是候选论元，将当前节点的父节点作为当前节点重复以上过程直至到根节点为止。\n",
    "\n",
    "基于依存句法分析结果的语义角色标注方法中的论元识别算法的特征设计也稍有不同，多了有关父子节点的一些特征。\n",
    "\n",
    "有了以上几种语义角色标注方法一定会各有优缺点，因此就有人想到了多种方法相融合的方法，融合的方式可以是：加权求和、插值……，最终效果肯定是更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐含语义索引模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从海量语料资源中找寻信息需要依赖于信息检索的方法，信息检索无论是谷歌还是百度都离不开TF-IDF算法，但TF-IDF是万能的吗？并不是，它简单有效但缺乏语义特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF(term frequency)，表示一个词在一个文档中出现的频率；IDF(inverse document frequency)，表示一个词出现在多少个文档中。\n",
    "\n",
    "它的思路是这样的：同一个词在短文档中出现的次数和在长文档中出现的次数一样多时，对于短文档价值更大；一个出现概率很低的词一旦出现在文档中，其价值应该大于其他普遍出现的词。\n",
    "\n",
    "这在信息检索领域的向量模型中做相似度计算非常有效，屡试不爽，曾经是google老大哥发家的必杀技。但是在开发聊天机器人这个事情上看到了它的软肋，那就是它只是考虑独立的词上的事情，并没有任何语义信息在里面，因此我们需要选择加入了语义特征的更有效的信息检索模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隐含语义索引模型\n",
    "\n",
    "在TF-IDF模型中，所有词构成一个高维的语义空间，每个文档在这个空间中被映射为一个点，这种方法维数一般比较高而且每个词作为一维割裂了词与词之间的关系。所以为了解决这个问题，我们要把词和文档同等对待，构造一个维数不高的语义空间，每个词和每个文档都是被映射到这个空间中的一个点。用数学来表示这个思想就是说，我们考察的概率即包括文档的概率，也包括词的概率，以及他们的联合概率。\n",
    "\n",
    "为了加入语义方面的信息，我们设计一个假想的隐含类包括在文档和词之间，具体思路是这样的：\n",
    "\n",
    "（1）选择一个文档的概率是p(d);\n",
    "\n",
    "（2）找到一个隐含类的概率是p(z|d);\n",
    "\n",
    "（3）生成一个词w的概率为p(w|z);\n",
    "\n",
    "以上是假设的条件概率，我们根据观测数据能估计出来的是p(d, w)联合概率，这里面的z是一个隐含变量，表达的是一种语义特征。那么我们要做的就是利用p(d, w)来估计p(d)、p(z|d)和p(w|z)，最终根据p(d)、p(z|d)和p(w|z)来求得更精确的p(w, d)，即词与文档之间的相关度。\n",
    "\n",
    "为了做更精确的估计，设计优化的目标函数是对数似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L=∑∑n(d, w) log P(d, w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如何来通过机器学习训练这些概率呢？首先我们知道：\n",
    "\n",
    "$p(d, w) = p(d) × p(w|d)$  \n",
    "\n",
    "而\n",
    "\n",
    "$p(w|d) = ∑p(w|z)p(z|d)$  \n",
    "\n",
    "同时又有：\n",
    "\n",
    "$p(z|d) = p(z)p(d|z)/∑p(z)p(d|z)$  \n",
    "\n",
    "那么\n",
    "\n",
    "$p(d, w) =p(d)×∑p(w|z) p(z)p(d|z)/∑p(z)p(d|z)=∑p(z)×p(w|z)×p(d|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们采取EM算法，EM算法的精髓就是按照最大似然的原理，先随便拍一个分布参数，让每个人都根据分布归类到某一部分，然后根据这些归类来重新统计数目，按照最大似然估计分布参数，然后再重新归类、调参、估计、归类、调参、估计，最终得出最优解\n",
    "\n",
    "那么我们要把每一个训练数据做归类，即p(z|d,w)，那么这个概率值怎么计算呢？\n",
    "\n",
    "我们先拍一个p(z)、p(d|z)、p(w|z)\n",
    "\n",
    "然后根据\n",
    "\n",
    "$p(z|d,w)=p(z)p(d|z)p(w|z)/∑p(z)p(d|z)p(w|z)$，其中分子是一个z，分母是所有的z的和   \n",
    "\n",
    "这样计算出来的值是p(z|d,w)的最大似然估计的概率估计（**这是E过程**）\n",
    "\n",
    "然后根据这个估计来对每一个训练样本做归类\n",
    "\n",
    "根据归类好的数据统计出n(d,w)\n",
    "\n",
    "然后我再根据以下公式来更新参数\n",
    "\n",
    "$p(z) = 1/R  ∑n(d,w)p(z|d,w)$\n",
    "\n",
    "$p(d|z)=∑n(d,w)p(z|d,w) / ∑n(d,w)p(z|d,w)$，其中分子是一个d的和，分母是所有的d的和，这样计算出来的值是p(d|z)的最大似然估计\n",
    "\n",
    "$p(w|z)=∑n(d,w)p(z|d,w) / ∑n(d,w)p(z|d,w)$，其中分子是一个w的和，分母是所有的w的和，这样计算出来的值是p(w|z)的最大似然估计\n",
    "\n",
    "\n",
    "最后重新计算p(z|d,w)：\n",
    "\n",
    "$p(z|d,w)=p(z)p(d|z)p(w|z)/∑p(z)p(d|z)p(w|z)$  \n",
    "\n",
    "**这是M的过程**\n",
    "\n",
    "不断重复上面EM的过程使得对数似然函数最大：\n",
    "\n",
    "$L=∑∑n(d, w) log P(d, w)$  \n",
    "\n",
    "通过以上迭代就能得出最终的p(w, d)，即词与文档之间的相关度，后面就是利用相关度做检索的过程了\n",
    "\n",
    "为了得到词词之间的相关度，我们用p(w, d)乘以它的转置，即\n",
    "\n",
    "$p(w,w) = p(w,d)×trans(p(w,d))$  \n",
    "\n",
    "当用户查询query的关键词构成词向量Wq, 而文档d表示成词向量Wd，那么query和文档d的相关度就是：\n",
    "\n",
    "$R(query, d) = Wq×p(w,w)×Wd$  \n",
    "\n",
    "这样把所有文档算出来的相关度从大到小排序就是搜索的排序结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
