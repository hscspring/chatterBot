{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#人工神经网络\" data-toc-modified-id=\"人工神经网络-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>人工神经网络</a></div><div class=\"lev2 toc-item\"><a href=\"#自编码\" data-toc-modified-id=\"自编码-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>自编码</a></div><div class=\"lev2 toc-item\"><a href=\"#深度学习\" data-toc-modified-id=\"深度学习-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>深度学习</a></div><div class=\"lev2 toc-item\"><a href=\"#CNN\" data-toc-modified-id=\"CNN-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>CNN</a></div><div class=\"lev1 toc-item\"><a href=\"#CNN-与深度学习\" data-toc-modified-id=\"CNN-与深度学习-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>CNN 与深度学习</a></div><div class=\"lev2 toc-item\"><a href=\"#卷积神经网络(CNN)\" data-toc-modified-id=\"卷积神经网络(CNN)-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>卷积神经网络(CNN)</a></div><div class=\"lev2 toc-item\"><a href=\"#多层卷积网络设计\" data-toc-modified-id=\"多层卷积网络设计-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>多层卷积网络设计</a></div><div class=\"lev2 toc-item\"><a href=\"#tensorflow代码实现\" data-toc-modified-id=\"tensorflow代码实现-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>tensorflow代码实现</a></div><div class=\"lev1 toc-item\"><a href=\"#深度学习-NLP\" data-toc-modified-id=\"深度学习-NLP-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>深度学习 NLP</a></div><div class=\"lev2 toc-item\"><a href=\"#词向量\" data-toc-modified-id=\"词向量-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>词向量</a></div><div class=\"lev2 toc-item\"><a href=\"#词向量如何训练得出呢？\" data-toc-modified-id=\"词向量如何训练得出呢？-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>词向量如何训练得出呢？</a></div><div class=\"lev2 toc-item\"><a href=\"#词向量的应用\" data-toc-modified-id=\"词向量的应用-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>词向量的应用</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工神经网络是借鉴了生物神经网络的工作原理形成的一种数学模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码\n",
    "\n",
    "我们这样来设计我们的神经网络：由n个输入特征得出与输入特征几乎相同的n个结果，这样训练出的隐藏层可以得到意想不到的信息。  \n",
    "\n",
    "比如，在信息检索领域，我们需要通过模型训练来得出合理的排序模型，那么输入的特征可能有：文档质量、文档点击历史、文档前链数目、文档锚文本信息……，为了能找出这些特征中隐藏的信息，我们把隐藏层的神经元数目设置的少于输入特征的数目，经过大量样本的训练出能还原原始特征的模型，这样相当于我们用少于输入特征数目的信息还原出了全部特征，表面上是一种压缩，实际上通过这种方式就可以发现某些特征之间存在隐含的相关性，或者有某种特殊的关系。\n",
    "\n",
    "同样的，我们还可以让隐藏层中的神经元数目多余输入特征的数目，这样经过训练得出的模型还可以展示出特征之间某种细节上的关联，比如我们对图像识别做这样的模型训练，在得出的隐藏层中能展示出多种特征之间的细节信息，如鼻子一定在嘴和眼睛中间。\n",
    "\n",
    "这种让输出和输入一致的用法就是传说中的自编码算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习\n",
    "\n",
    "人工神经网络模型通过多层神经元结构建立而成，每一层可以抽象为一种思维过程，经过多层思考，最终得出结论。举一个实际的例子：识别美女图片\n",
    "\n",
    "按照人的思维过程，识别美女图片要经过这样的判断：1）图片类别（人物、风景……）；2）图片人物性别（男、女、其他……）；3）相貌如何（美女、恐龙、5分……）\n",
    "\n",
    "那么在人工神经网络中，这个思考过程可以抽象成多个层次的计算：第一层计算提取图片中有关类别的特征，比如是否有形如耳鼻口手的元素，是否有形如蓝天白云绿草地的元素；第二层提取是否有胡须、胸部、长发以及面部特征等来判断性别；第三层提取五官、肤质、衣着等信息来确定颜值。为了让神经网络每一层有每一层专门要做的事情，需要在每一层的神经元中添加特殊的约束条件才能做到。人类的大脑是经过上亿年进化而成的，它的功能深不可及，某些效率也极高，而计算机在某些方面效率比人脑要高很多，两种结合起来一切皆有可能。\n",
    "\n",
    "这种通过很多层提取特定特征来做机器学习的方法就是传说中的深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN \n",
    "\n",
    "讲述第三种用法之前我们先讲一下什么是卷积运算。卷积英文是convolution(英文含义是：盘绕、弯曲、错综复杂)，数学表达是：\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} f(\\tau)g(x-\\tau)d\\tau$\n",
    "\n",
    " \n",
    "\n",
    "数学上不好理解，我们可以通俗点来讲：卷积就相当于在一定范围内做平移并求平均值。比如说回声可以理解为原始声音的卷积结果，因为回声是原始声音经过很多物体反射回来声音揉在一起。再比如说回声可以理解为把信号分解成无穷多的冲击信号，然后再进行冲击响应的叠加。再比如说把一张图像做卷积运算，并把计算结果替换原来的像素点，可以实现一种特殊的模糊，这种模糊其实是一种新的特征提取，提取的特征就是图像的纹路。总之卷积就是先打乱，再叠加。\n",
    "\n",
    "下面我们在看上面的积分公式，需要注意的是这里是对τ积分，不是对x积分。也就是说对于固定的x，找到x附近的所有变量，求两个函数的乘积，并求和。\n",
    "\n",
    "下面回归正题，在神经网络里面，我们设计每个神经元计算输出的公式是卷积公式，这样相当于神经网络的每一层都会输出一种更高级的特征，比如说形状、脸部轮廓等。这种神经网络叫做卷积神经网络。\n",
    "\n",
    "继续深入主题，在自然语言中，我们知道较近的上下文词语之间存在一定的相关性，由于标点、特殊词等的分隔使得在传统自然语言处理中会脱离词与词之间的关联，结果丢失了一部分重要信息，利用卷积神经网络完全可以做多元(n-gram)的计算，不会损失自然语言中的临近词的相关性信息。这种方法对于语义分析、语义聚类等都有非常好的效果。\n",
    "\n",
    "这种神奇用法就是传说中的CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 与深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动问答系统中深度学习的应用较多是RNN，这归因于它天然利用时序建模。俗话说知己知彼百战不殆，为了理解RNN，我们先来了解一下CNN，通过手写数字识别案例来感受一下CNN最擅长的局部感知能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络(CNN)\n",
    "\n",
    "卷积神经网络(Convolutional Neural Network,CNN)是将二维离散卷积运算和人工神经网络相结合的一种深度神经网络。它的特点是可以自动提取特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层卷积网络设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对mnist手写数据集做训练，我们设计这样的多层卷积网络：\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/3941ea72a9fbca647e1562e3a569228d3921db23.png)\n",
    "\n",
    "第一层由一个卷积和一个max pooling完成，其中卷积运算的“视野”是5×5的像素范围，卷积使用1步长、0边距的模板(保证输入输出是同一个大小)，1个输入通道(因为图片是灰度的，单色)，32个输出通道(也就是设计32个特征)。由于我们通过上面read_images.c的打印可以看到每张图片都是28×28像素，那么第一次卷积输出也是28×28大小。max pooling采用2×2大小的模板，那么池化后输出的尺寸就是14×14，因为一共有32个通道，所以一张图片的输出一共是14×14×32=6272像素\n",
    "\n",
    "第二层同样由一个卷积和一个max pooling完成，和第一层不同的是输入通道有32个(对应第一层的32个特征)，输出通道我们设计64个(即输出64个特征)，因为这一层的输入是每张大小14×14，所以这一个卷积层输出也是14×14，再经过这一层max pooling，输出大小就是7×7，那么一共输出像素就是7×7×64=3136\n",
    "\n",
    "第三层是一个密集连接层，我们设计一个有1024个神经元的全连接层，这样就相当于第二层输出的7×7×64个值都作为这1024个神经元的输入\n",
    "\n",
    "为了让算法更“智能”，我们把这些神经元的激活函数设计为ReLu函数，即如下图像中的蓝色(其中绿色是它的平滑版g(x)=log(1+e^x))：  \n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/79abd1f246da3600c49a14650843fcf5f3633d6b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding( \"utf-8\" )\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('data_dir', './', 'Directory for storing data')\n",
    "\n",
    "mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "# 初始化生成随机的权重(变量)，避免神经元输出恒为0\n",
    "def weight_variable(shape):\n",
    "    # 以正态分布生成随机值\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 初始化生成随机的偏置项(常量)，避免神经元输出恒为0\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积采用1步长，0边距，保证输入输出大小相同\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 池化采用2×2模板\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# 输出类别共10个：0-9\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "# 第一层卷积权重，视野是5*5，输入通道1个，输出通道32个\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# 第一层卷积偏置项有32个\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# 把x变成4d向量，第二维和第三维是图像尺寸，第四维是颜色通道数1\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 第二层卷积权重，视野是5*5，输入通道32个，输出通道64个\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# 第二层卷积偏置项有64个\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 第二层池化后尺寸编程7*7，第三层是全连接，输入是64个通道，输出是1024个神经元\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "# 第三层全连接偏置项有1024个\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# 按float做dropout，以减少过拟合\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# 最后的softmax层生成10种分类\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "# Adam优化器来做梯度最速下降\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print \"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习 NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于语言相比于语音、图像来说，是一种更高层的抽象，因此不是那么适合于深度学习，但是经过人类不断探索，也发现无论多么高层的抽象总是能通过更多底层基础的累积而碰触的到，本文介绍如何将深度学习应用到NLP所必须的底层基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量\n",
    "\n",
    "自然语言需要数学化才能够被计算机认识和计算。数学化的方法有很多，最简单的方法是为每个词分配一个编号，这种方法已经有多种应用，但是依然存在一个缺点：不能表示词与词的关系。\n",
    "\n",
    "词向量是这样的一种向量[0.1, -3.31, 83.37, 93.0, -18.37, ……]，每一个词对应一个向量，词义相近的词，他们的词向量距离也会越近(欧氏距离、夹角余弦)\n",
    "\n",
    "词向量有一个优点，就是维度一般较低，一般是50维或100维，这样可以避免维度灾难，也更容易使用深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量如何训练得出呢？\n",
    "\n",
    "语言模型表达的实际就是已知前n-1个词的前提下，预测第n个词的概率。\n",
    "\n",
    "词向量的训练是一种无监督学习，也就是没有标注数据，给我n篇文章，我就可以训练出词向量。\n",
    "\n",
    "基于三层神经网络构建n-gram语言模型(词向量顺带着就算出来了)的基本思路：  \n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/436984a952d8fb45270875d8452081517d149973.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最下面的w是词，其上面的C(w)是词向量，词向量一层也就是神经网络的输入层(第一层)，这个输入层是一个(n-1)×m的矩阵，其中n-1是词向量数目，m是词向量维度\n",
    "\n",
    "第二层(隐藏层)是就是普通的神经网络，以H为权重，以tanh为激活函数\n",
    "\n",
    "第三层(输出层)有|V|个节点，|V|就是词表的大小，输出以U为权重，以softmax作为激活函数以实现归一化，最终就是输出可能是某个词的概率。\n",
    "\n",
    "另外，神经网络中有一个技巧就是增加一个从输入层到输出层的直连边(线性变换)，这样可以提升模型效果，这个变换矩阵设为W\n",
    "\n",
    "假设C(w)就是输入的x，那么y的计算公式就是y = b + Wx + Utanh(d+Hx)\n",
    "\n",
    "这个模型里面需要训练的有这么几个变量：C、H、U、W。利用梯度下降法训练之后得出的C就是生成词向量所用的矩阵，C(w)表示的就是我们需要的词向量\n",
    "\n",
    "上面是讲解词向量如何“顺带”训练出来的，然而真正有用的地方在于这个词向量如何进一步应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量的应用\n",
    "\n",
    "第一种应用是找同义词。具体应用案例就是google的word2vec工具，通过训练好的词向量，指定一个词，可以返回和它cos距离最相近的词并排序。\n",
    "\n",
    "第二种应用是词性标注和语义角色标注任务。具体使用方法是：把词向量作为神经网络的输入层，通过前馈网络和卷积网络完成。\n",
    "\n",
    "第三种应用是句法分析和情感分析任务。具体使用方法是：把词向量作为递归神经网络的输入。\n",
    "\n",
    "第四种应用是命名实体识别和短语识别。具体使用方法是：把词向量作为扩展特征使用。\n",
    "\n",
    "另外词向量有一个非常特别的现象：C(king)-C(queue)≈C(man)-C(woman)，这里的减法就是向量逐维相减，换个表达方式就是：C(king)-C(man)+C(woman)和它最相近的向量就是C(queue)，这里面的原理其实就是：语义空间中的线性关系。基于这个结论相信会有更多奇妙的功能出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
