{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#聊天机器人怎么做\" data-toc-modified-id=\"聊天机器人怎么做-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>聊天机器人怎么做</a></div><div class=\"lev2 toc-item\"><a href=\"#工作原理\" data-toc-modified-id=\"工作原理-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>工作原理</a></div><div class=\"lev2 toc-item\"><a href=\"#关键技术\" data-toc-modified-id=\"关键技术-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>关键技术</a></div><div class=\"lev2 toc-item\"><a href=\"#技术方法\" data-toc-modified-id=\"技术方法-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>技术方法</a></div><div class=\"lev1 toc-item\"><a href=\"#词性标注与关键词提取\" data-toc-modified-id=\"词性标注与关键词提取-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>词性标注与关键词提取</a></div><div class=\"lev2 toc-item\"><a href=\"#问句解析过程\" data-toc-modified-id=\"问句解析过程-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>问句解析过程</a></div><div class=\"lev2 toc-item\"><a href=\"#NLPIR-使用\" data-toc-modified-id=\"NLPIR-使用-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>NLPIR 使用</a></div><div class=\"lev1 toc-item\"><a href=\"#0-字节存储海量语料资源\" data-toc-modified-id=\"0-字节存储海量语料资源-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>0 字节存储海量语料资源</a></div><div class=\"lev2 toc-item\"><a href=\"#搜索引擎\" data-toc-modified-id=\"搜索引擎-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>搜索引擎</a></div><div class=\"lev2 toc-item\"><a href=\"#语料提取\" data-toc-modified-id=\"语料提取-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>语料提取</a></div><div class=\"lev1 toc-item\"><a href=\"#利用中文语言技术平台做依存句法和语义依存分析\" data-toc-modified-id=\"利用中文语言技术平台做依存句法和语义依存分析-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>利用中文语言技术平台做依存句法和语义依存分析</a></div><div class=\"lev2 toc-item\"><a href=\"#依存句法分析\" data-toc-modified-id=\"依存句法分析-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>依存句法分析</a></div><div class=\"lev2 toc-item\"><a href=\"#语义依存分析\" data-toc-modified-id=\"语义依存分析-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>语义依存分析</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聊天机器人怎么做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三个模块\n",
    "  - 提问处理模块\n",
    "    - 查询关键词生成\n",
    "    - 答案类型确定\n",
    "    - 句法和语义分析\n",
    "  - 检索模块\n",
    "    - 根据查询关键词进行信息检索，返回句子或段落\n",
    "  - 答案抽取模块\n",
    "    - 分析和推理从检索的句子或段落中抽取出和提问一致的实体\n",
    "    - 根据概率最大对候选答案排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 海量文本知识表示\n",
    "  - 网络文本获取\n",
    "  - 机器学习方法\n",
    "  - 大规模语义计算和推理\n",
    "  - 知识表示体系\n",
    "  - 知识库构建\n",
    "- 问句解析\n",
    "  - 中文分词\n",
    "  - 词性标注\n",
    "  - 实体标注\n",
    "  - 概念类别标注\n",
    "  - 句法分析\n",
    "  - 语义分析\n",
    "  - 逻辑结构标注\n",
    "  - 指代消解\n",
    "  - 关联关系标注\n",
    "  - 问句分类\n",
    "    - 简单问句\n",
    "    - 复杂问句\n",
    "    - 实体型\n",
    "    - 段落型\n",
    "    - 篇章级\n",
    "  - 答案类别确定\n",
    "- 答案生成与过滤\n",
    "  - 候选答案抽取\n",
    "  - 关系推演\n",
    "    - 并列\n",
    "    - 递进\n",
    "    - 因果\n",
    "  - 吻合程度判断\n",
    "  - 噪声过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 技术方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于检索的技术\n",
    "- 基于模式匹配的技术\n",
    "- 基于自然语言理解的技术\n",
    "- 基于统计翻译模型的技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注与关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问句解析过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般问句解析需要进行分词、词性标注、命名实体识别、关键词提取、句法分析以及查询问句分类等\n",
    "\n",
    "- 第一个要数哈工大的LTP(语言技术平台)了，它可以做中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等丰富、 高效、精准的自然语言处理技术\n",
    "\n",
    "- 第二个就是博森科技了，它除了做中文分词、词性标注、命名实体识别、依存文法之外还可以做情感分析、关键词提取、新闻分类、语义联想、时间转换、新闻摘要等，但因为是商业化的公司，除了分词和词性标注免费之外全都收费\n",
    "\n",
    "- 第三个就是jieba分词，这个开源小工具分词和词性标注做的挺不错的，但是其他方面还欠缺一下，如果只是中文分词的需求完全可以满足\n",
    "\n",
    "- 第四个就是中科院张华平博士的NLPIR汉语分词系统，也能支持关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPIR 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "s = '聊天机器人到底该怎么做呢？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segments = pseg.cut(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聊天/nz\n",
      "机器人/n\n",
      "到底/d\n",
      "该/r\n",
      "怎么/r\n",
      "做/v\n",
      "呢/y\n",
      "？/x\n"
     ]
    }
   ],
   "source": [
    "for w in segments:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\u673a\\u5668\\u4eba', u'\\u505a']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.textrank(s, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机器人\n"
     ]
    }
   ],
   "source": [
    "print(u'\\u673a\\u5668\\u4eba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "做\n"
     ]
    }
   ],
   "source": [
    "print( u'\\u505a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\u673a\\u5668\\u4eba', u'\\u505a']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.textrank(s, topK=20, withWeight=False,allowPOS=('ns', 'n', 'vn', 'v', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in _:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr4w = TextRank4Keyword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr4w.analyze(text=s, lower=True, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weight': 0.4651161545800183, 'word': u'\\u804a\\u5929'},\n",
       " {'weight': 0.4651161545800183, 'word': u'\\u673a\\u5668\\u4eba'},\n",
       " {'weight': 0.06976769083996354, 'word': u'\\u505a'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr4w.get_keywords(20, word_min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in tr4w.get_keywords(20, word_min_len=1):\n",
    "    print(item.word, item.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(u'\\u804a\\u5929') # 聊天"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "方案\n",
    "- jieba 分词 + 词性标注\n",
    "- TextRank 关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 字节存储海量语料资源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 搜索引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import sys \n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class BaiduSearchSpider(scrapy.Spider):\n",
    "    name = \"badu_search\"\n",
    "    allowed_domains = [\"baidu.com\"]\n",
    "    start_urls = [\n",
    "                \"http://www.baidu.com/s?wd=机器学习\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        print response.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "\n",
    "import sys \n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "import scrapy\n",
    "from w3lib.html import remove_tags\n",
    "#from scrapy.selector import Selector\n",
    "#from scrapy.http import HtmlResponse\n",
    "\n",
    "class BaiduSearchSpider(scrapy.Spider):\n",
    "    name = \"baidu_search\"\n",
    "    allowed_domains = [\"baidu.com\"]\n",
    "    start_urls = [\n",
    "                \"http://www.baidu.com/s?wd=机器学习\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "    \threfs = response.selector.xpath('//div[contains(@class, \"c-container\")]/h3/a/@href').extract()\n",
    "    \tcontainers = response.selector.xpath('//div[contains(@class, \"c-container\")]')\n",
    "    \tfor container in containers:\n",
    "    \t\thref = container.xpath('h3/a/@href').extract()[0]\n",
    "    \t\ttitle = remove_tags(container.xpath('h3/a').extract()[0])\n",
    "    \t\tc_abstract = container.xpath('div/div/div[contains(@class, \"c-abstract\")]').extract()\n",
    "    \t\tabstract = \"\"\n",
    "    \t\tif len(c_abstract) > 0:\n",
    "    \t\t\tabstract = remove_tags(c_abstract[0])\n",
    "    \t\trequest = scrapy.Request(href, callback=self.parse_url)\n",
    "    \t\trequest.meta['title'] = title\n",
    "    \t\trequest.meta['abstract'] = abstract\n",
    "    \t\tyield request\n",
    "    \t#for href in hrefs:\n",
    "    \t#\tyield scrapy.Request(href, callback=self.parse_url)\n",
    "    def parse_url(self, response):\n",
    "    \tprint \"url:\", response.url\n",
    "    \tprint \"title\", response.meta['title']\n",
    "    \tprint \"abstract\", response.meta['abstract']\n",
    "    \tcontent = remove_tags(response.selector.xpath('//body').extract()[0])\n",
    "    \tprint \"content_len\", len(content)\n",
    "\n",
    "    \t#print len(response.body)\n",
    "    \t#print remove_tags(response.selector.xpath('//body').extract()[0])\n",
    "\n",
    "\"\"\"\n",
    "    def parse(self, response):\n",
    "    \tprint response.body\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用中文语言技术平台做依存句法和语义依存分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依存句法分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句法就是句子的法律规则，也就是句子里成分都是按照什么法律规则组织在一起的。而依存句法就是这些成分之间有一种依赖关系。什么是依赖：没有你的话，我存在就是个错误。“北京是中国的首都”，如果没有“首都”，那么“中国的”存在就是个错误，因为“北京是中国的”表达的完全是另外一个意思了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依存句法分析的基本任务是确定句式的句法结构(短语结构)或句子中词汇之间的依存关系。依存句法分析最重要的两棵树：\n",
    "\n",
    "依存树：子节点依存于父节点\n",
    "\n",
    "依存投射树：实线表示依存联结关系，位置低的成分依存于位置高的成分，虚线为投射线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**依存关系的五条公理**\n",
    "\n",
    "1. 一个句子中只有一个成分是独立的\n",
    "\n",
    "2. 其他成分直接依存于某一成分\n",
    "\n",
    "3. 任何一个成分都不能依存于两个或两个以上的成分\n",
    "\n",
    "4. 如果A成分直接依存于B成分，而C成分在句子中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分\n",
    "\n",
    "5. 中心成分左右两面的其他成分相互不发生关系\n",
    "\n",
    " \n",
    "\n",
    "什么地方存在依存关系呢？比如合成词（如：国内）、短语（如：英雄联盟）很多地方都是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语义依存分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“语义”就是说句子的含义，“张三昨天告诉李四一个秘密”，那么语义包括：谁告诉李四秘密的？张三。张三告诉谁一个秘密？李四。张三什么时候告诉的？昨天。张三告诉李四什么？秘密。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依存句法强调介词、助词等的划分作用，语义依存注重实词之间的逻辑关系\n",
    "\n",
    "另外，依存句法随着字面词语变化而不同，语义依存不同字面词语可以表达同一个意思，句法结构不同的句子语义关系可能相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -i \"http://api.ltp-cloud.com/analysis/?api_key=r1W0j2O4006d5QsmRVVDqz6bJMXkfT5NXbjEYkHp&text=我是中国人。&pattern=dp&format=plain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -i --data-urlencode text=\"我是中国人。\" -d \"api_key=r1W0j2O4006d5QsmRVVDqz6bJMXkfT5NXbjEYkHp&pattern=dp&format=plain\" \"http://api.ltp-cloud.com/analysis/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
