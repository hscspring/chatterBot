{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#人工神经网络\" data-toc-modified-id=\"人工神经网络-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>人工神经网络</a></div><div class=\"lev2 toc-item\"><a href=\"#自编码\" data-toc-modified-id=\"自编码-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>自编码</a></div><div class=\"lev2 toc-item\"><a href=\"#深度学习\" data-toc-modified-id=\"深度学习-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>深度学习</a></div><div class=\"lev2 toc-item\"><a href=\"#CNN\" data-toc-modified-id=\"CNN-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>CNN</a></div><div class=\"lev1 toc-item\"><a href=\"#CNN-与深度学习\" data-toc-modified-id=\"CNN-与深度学习-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>CNN 与深度学习</a></div><div class=\"lev2 toc-item\"><a href=\"#卷积神经网络(CNN)\" data-toc-modified-id=\"卷积神经网络(CNN)-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>卷积神经网络(CNN)</a></div><div class=\"lev2 toc-item\"><a href=\"#多层卷积网络设计\" data-toc-modified-id=\"多层卷积网络设计-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>多层卷积网络设计</a></div><div class=\"lev2 toc-item\"><a href=\"#tensorflow代码实现\" data-toc-modified-id=\"tensorflow代码实现-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>tensorflow代码实现</a></div><div class=\"lev1 toc-item\"><a href=\"#深度学习-NLP\" data-toc-modified-id=\"深度学习-NLP-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>深度学习 NLP</a></div><div class=\"lev2 toc-item\"><a href=\"#词向量\" data-toc-modified-id=\"词向量-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>词向量</a></div><div class=\"lev2 toc-item\"><a href=\"#词向量如何训练得出呢？\" data-toc-modified-id=\"词向量如何训练得出呢？-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>词向量如何训练得出呢？</a></div><div class=\"lev2 toc-item\"><a href=\"#词向量的应用\" data-toc-modified-id=\"词向量的应用-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>词向量的应用</a></div><div class=\"lev1 toc-item\"><a href=\"#word2vec\" data-toc-modified-id=\"word2vec-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>word2vec</a></div><div class=\"lev2 toc-item\"><a href=\"#localist-representation与distributed-representation\" data-toc-modified-id=\"localist-representation与distributed-representation-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>localist representation与distributed representation</a></div><div class=\"lev2 toc-item\"><a href=\"#word-embedding\" data-toc-modified-id=\"word-embedding-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>word embedding</a></div><div class=\"lev2 toc-item\"><a href=\"#word2vec中的神经网络\" data-toc-modified-id=\"word2vec中的神经网络-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>word2vec中的神经网络</a></div><div class=\"lev2 toc-item\"><a href=\"#CBOW模型\" data-toc-modified-id=\"CBOW模型-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>CBOW模型</a></div><div class=\"lev2 toc-item\"><a href=\"#基于霍夫曼树的Hierarchical-Softmax技术\" data-toc-modified-id=\"基于霍夫曼树的Hierarchical-Softmax技术-45\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>基于霍夫曼树的Hierarchical Softmax技术</a></div><div class=\"lev2 toc-item\"><a href=\"#Skip-gram模型\" data-toc-modified-id=\"Skip-gram模型-46\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Skip-gram模型</a></div><div class=\"lev1 toc-item\"><a href=\"#递归神经网络-RNN\" data-toc-modified-id=\"递归神经网络-RNN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>递归神经网络 RNN</a></div><div class=\"lev2 toc-item\"><a href=\"#递归神经网络\" data-toc-modified-id=\"递归神经网络-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>递归神经网络</a></div><div class=\"lev2 toc-item\"><a href=\"#时间递归神经网络\" data-toc-modified-id=\"时间递归神经网络-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>时间递归神经网络</a></div><div class=\"lev2 toc-item\"><a href=\"#RNN-训练方法\" data-toc-modified-id=\"RNN-训练方法-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>RNN 训练方法</a></div><div class=\"lev2 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-54\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev1 toc-item\"><a href=\"#深度学习自动问答的一般方法\" data-toc-modified-id=\"深度学习自动问答的一般方法-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>深度学习自动问答的一般方法</a></div><div class=\"lev2 toc-item\"><a href=\"#语料库的获取方法\" data-toc-modified-id=\"语料库的获取方法-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>语料库的获取方法</a></div><div class=\"lev2 toc-item\"><a href=\"#基于CNN的系统设计\" data-toc-modified-id=\"基于CNN的系统设计-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>基于CNN的系统设计</a></div><div class=\"lev2 toc-item\"><a href=\"#通用的训练方法\" data-toc-modified-id=\"通用的训练方法-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>通用的训练方法</a></div><div class=\"lev2 toc-item\"><a href=\"#神经网络结构设计\" data-toc-modified-id=\"神经网络结构设计-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>神经网络结构设计</a></div><div class=\"lev2 toc-item\"><a href=\"#总结\" data-toc-modified-id=\"总结-65\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>总结</a></div><div class=\"lev1 toc-item\"><a href=\"#基于美剧字幕的聊天语料库建设方案\" data-toc-modified-id=\"基于美剧字幕的聊天语料库建设方案-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>基于美剧字幕的聊天语料库建设方案</a></div><div class=\"lev2 toc-item\"><a href=\"#美剧字幕\" data-toc-modified-id=\"美剧字幕-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>美剧字幕</a></div><div class=\"lev2 toc-item\"><a href=\"#自动抓取字幕\" data-toc-modified-id=\"自动抓取字幕-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>自动抓取字幕</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工神经网络是借鉴了生物神经网络的工作原理形成的一种数学模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码\n",
    "\n",
    "我们这样来设计我们的神经网络：由n个输入特征得出与输入特征几乎相同的n个结果，这样训练出的隐藏层可以得到意想不到的信息。  \n",
    "\n",
    "比如，在信息检索领域，我们需要通过模型训练来得出合理的排序模型，那么输入的特征可能有：文档质量、文档点击历史、文档前链数目、文档锚文本信息……，为了能找出这些特征中隐藏的信息，我们把隐藏层的神经元数目设置的少于输入特征的数目，经过大量样本的训练出能还原原始特征的模型，这样相当于我们用少于输入特征数目的信息还原出了全部特征，表面上是一种压缩，实际上通过这种方式就可以发现某些特征之间存在隐含的相关性，或者有某种特殊的关系。\n",
    "\n",
    "同样的，我们还可以让隐藏层中的神经元数目多余输入特征的数目，这样经过训练得出的模型还可以展示出特征之间某种细节上的关联，比如我们对图像识别做这样的模型训练，在得出的隐藏层中能展示出多种特征之间的细节信息，如鼻子一定在嘴和眼睛中间。\n",
    "\n",
    "这种让输出和输入一致的用法就是传说中的自编码算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习\n",
    "\n",
    "人工神经网络模型通过多层神经元结构建立而成，每一层可以抽象为一种思维过程，经过多层思考，最终得出结论。举一个实际的例子：识别美女图片\n",
    "\n",
    "按照人的思维过程，识别美女图片要经过这样的判断：1）图片类别（人物、风景……）；2）图片人物性别（男、女、其他……）；3）相貌如何（美女、恐龙、5分……）\n",
    "\n",
    "那么在人工神经网络中，这个思考过程可以抽象成多个层次的计算：第一层计算提取图片中有关类别的特征，比如是否有形如耳鼻口手的元素，是否有形如蓝天白云绿草地的元素；第二层提取是否有胡须、胸部、长发以及面部特征等来判断性别；第三层提取五官、肤质、衣着等信息来确定颜值。为了让神经网络每一层有每一层专门要做的事情，需要在每一层的神经元中添加特殊的约束条件才能做到。人类的大脑是经过上亿年进化而成的，它的功能深不可及，某些效率也极高，而计算机在某些方面效率比人脑要高很多，两种结合起来一切皆有可能。\n",
    "\n",
    "这种通过很多层提取特定特征来做机器学习的方法就是传说中的深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN \n",
    "\n",
    "讲述第三种用法之前我们先讲一下什么是卷积运算。卷积英文是convolution(英文含义是：盘绕、弯曲、错综复杂)，数学表达是：\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} f(\\tau)g(x-\\tau)d\\tau$\n",
    "\n",
    " \n",
    "\n",
    "数学上不好理解，我们可以通俗点来讲：卷积就相当于在一定范围内做平移并求平均值。比如说回声可以理解为原始声音的卷积结果，因为回声是原始声音经过很多物体反射回来声音揉在一起。再比如说回声可以理解为把信号分解成无穷多的冲击信号，然后再进行冲击响应的叠加。再比如说把一张图像做卷积运算，并把计算结果替换原来的像素点，可以实现一种特殊的模糊，这种模糊其实是一种新的特征提取，提取的特征就是图像的纹路。总之卷积就是先打乱，再叠加。\n",
    "\n",
    "下面我们在看上面的积分公式，需要注意的是这里是对τ积分，不是对x积分。也就是说对于固定的x，找到x附近的所有变量，求两个函数的乘积，并求和。\n",
    "\n",
    "下面回归正题，在神经网络里面，我们设计每个神经元计算输出的公式是卷积公式，这样相当于神经网络的每一层都会输出一种更高级的特征，比如说形状、脸部轮廓等。这种神经网络叫做卷积神经网络。\n",
    "\n",
    "继续深入主题，在自然语言中，我们知道较近的上下文词语之间存在一定的相关性，由于标点、特殊词等的分隔使得在传统自然语言处理中会脱离词与词之间的关联，结果丢失了一部分重要信息，利用卷积神经网络完全可以做多元(n-gram)的计算，不会损失自然语言中的临近词的相关性信息。这种方法对于语义分析、语义聚类等都有非常好的效果。\n",
    "\n",
    "这种神奇用法就是传说中的CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 与深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动问答系统中深度学习的应用较多是RNN，这归因于它天然利用时序建模。俗话说知己知彼百战不殆，为了理解RNN，我们先来了解一下CNN，通过手写数字识别案例来感受一下CNN最擅长的局部感知能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络(CNN)\n",
    "\n",
    "卷积神经网络(Convolutional Neural Network,CNN)是将二维离散卷积运算和人工神经网络相结合的一种深度神经网络。它的特点是可以自动提取特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层卷积网络设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对mnist手写数据集做训练，我们设计这样的多层卷积网络：\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/3941ea72a9fbca647e1562e3a569228d3921db23.png)\n",
    "\n",
    "第一层由一个卷积和一个max pooling完成，其中卷积运算的“视野”是5×5的像素范围，卷积使用1步长、0边距的模板(保证输入输出是同一个大小)，1个输入通道(因为图片是灰度的，单色)，32个输出通道(也就是设计32个特征)。由于我们通过上面read_images.c的打印可以看到每张图片都是28×28像素，那么第一次卷积输出也是28×28大小。max pooling采用2×2大小的模板，那么池化后输出的尺寸就是14×14，因为一共有32个通道，所以一张图片的输出一共是14×14×32=6272像素\n",
    "\n",
    "第二层同样由一个卷积和一个max pooling完成，和第一层不同的是输入通道有32个(对应第一层的32个特征)，输出通道我们设计64个(即输出64个特征)，因为这一层的输入是每张大小14×14，所以这一个卷积层输出也是14×14，再经过这一层max pooling，输出大小就是7×7，那么一共输出像素就是7×7×64=3136\n",
    "\n",
    "第三层是一个密集连接层，我们设计一个有1024个神经元的全连接层，这样就相当于第二层输出的7×7×64个值都作为这1024个神经元的输入\n",
    "\n",
    "为了让算法更“智能”，我们把这些神经元的激活函数设计为ReLu函数，即如下图像中的蓝色(其中绿色是它的平滑版g(x)=log(1+e^x))：  \n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/79abd1f246da3600c49a14650843fcf5f3633d6b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding( \"utf-8\" )\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('data_dir', './', 'Directory for storing data')\n",
    "\n",
    "mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "# 初始化生成随机的权重(变量)，避免神经元输出恒为0\n",
    "def weight_variable(shape):\n",
    "    # 以正态分布生成随机值\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 初始化生成随机的偏置项(常量)，避免神经元输出恒为0\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积采用1步长，0边距，保证输入输出大小相同\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 池化采用2×2模板\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# 输出类别共10个：0-9\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "# 第一层卷积权重，视野是5*5，输入通道1个，输出通道32个\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# 第一层卷积偏置项有32个\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# 把x变成4d向量，第二维和第三维是图像尺寸，第四维是颜色通道数1\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 第二层卷积权重，视野是5*5，输入通道32个，输出通道64个\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# 第二层卷积偏置项有64个\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 第二层池化后尺寸编程7*7，第三层是全连接，输入是64个通道，输出是1024个神经元\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "# 第三层全连接偏置项有1024个\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# 按float做dropout，以减少过拟合\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# 最后的softmax层生成10种分类\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "# Adam优化器来做梯度最速下降\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print \"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习 NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于语言相比于语音、图像来说，是一种更高层的抽象，因此不是那么适合于深度学习，但是经过人类不断探索，也发现无论多么高层的抽象总是能通过更多底层基础的累积而碰触的到，本文介绍如何将深度学习应用到NLP所必须的底层基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量\n",
    "\n",
    "自然语言需要数学化才能够被计算机认识和计算。数学化的方法有很多，最简单的方法是为每个词分配一个编号，这种方法已经有多种应用，但是依然存在一个缺点：不能表示词与词的关系。\n",
    "\n",
    "词向量是这样的一种向量[0.1, -3.31, 83.37, 93.0, -18.37, ……]，每一个词对应一个向量，词义相近的词，他们的词向量距离也会越近(欧氏距离、夹角余弦)\n",
    "\n",
    "词向量有一个优点，就是维度一般较低，一般是50维或100维，这样可以避免维度灾难，也更容易使用深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量如何训练得出呢？\n",
    "\n",
    "语言模型表达的实际就是已知前n-1个词的前提下，预测第n个词的概率。\n",
    "\n",
    "词向量的训练是一种无监督学习，也就是没有标注数据，给我n篇文章，我就可以训练出词向量。\n",
    "\n",
    "基于三层神经网络构建n-gram语言模型(词向量顺带着就算出来了)的基本思路：  \n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/436984a952d8fb45270875d8452081517d149973.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最下面的w是词，其上面的C(w)是词向量，词向量一层也就是神经网络的输入层(第一层)，这个输入层是一个(n-1)×m的矩阵，其中n-1是词向量数目，m是词向量维度\n",
    "\n",
    "第二层(隐藏层)是就是普通的神经网络，以H为权重，以tanh为激活函数\n",
    "\n",
    "第三层(输出层)有|V|个节点，|V|就是词表的大小，输出以U为权重，以softmax作为激活函数以实现归一化，最终就是输出可能是某个词的概率。\n",
    "\n",
    "另外，神经网络中有一个技巧就是增加一个从输入层到输出层的直连边(线性变换)，这样可以提升模型效果，这个变换矩阵设为W\n",
    "\n",
    "假设C(w)就是输入的x，那么y的计算公式就是y = b + Wx + Utanh(d+Hx)\n",
    "\n",
    "这个模型里面需要训练的有这么几个变量：C、H、U、W。利用梯度下降法训练之后得出的C就是生成词向量所用的矩阵，C(w)表示的就是我们需要的词向量\n",
    "\n",
    "上面是讲解词向量如何“顺带”训练出来的，然而真正有用的地方在于这个词向量如何进一步应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量的应用\n",
    "\n",
    "第一种应用是找同义词。具体应用案例就是google的word2vec工具，通过训练好的词向量，指定一个词，可以返回和它cos距离最相近的词并排序。\n",
    "\n",
    "第二种应用是词性标注和语义角色标注任务。具体使用方法是：把词向量作为神经网络的输入层，通过前馈网络和卷积网络完成。\n",
    "\n",
    "第三种应用是句法分析和情感分析任务。具体使用方法是：把词向量作为递归神经网络的输入。\n",
    "\n",
    "第四种应用是命名实体识别和短语识别。具体使用方法是：把词向量作为扩展特征使用。\n",
    "\n",
    "另外词向量有一个非常特别的现象：C(king)-C(queue)≈C(man)-C(woman)，这里的减法就是向量逐维相减，换个表达方式就是：C(king)-C(man)+C(woman)和它最相近的向量就是C(queue)，这里面的原理其实就是：语义空间中的线性关系。基于这个结论相信会有更多奇妙的功能出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## localist representation与distributed representation  \n",
    "\n",
    "\n",
    "localist representation中文释义是稀疏表达，典型的案例就是one hot vector，也就是这样的一种向量表示：\n",
    "\n",
    "[1, 0, 0, 0, 0, 0……]表示成年男子\n",
    "\n",
    "[0, 1, 0, 0, 0, 0……]表示成年女子\n",
    "\n",
    "[0, 0, 1, 0, 0, 0……]表示老爷爷\n",
    "\n",
    "[0, 0, 0, 1, 0, 0……]表示老奶奶\n",
    "\n",
    "[0, 0, 0, 0, 1, 0……]表示男婴\n",
    "\n",
    "[0, 0, 0, 0, 0, 1……]表示女婴\n",
    "\n",
    "……\n",
    "\n",
    "每一类型用向量中的一维来表示\n",
    "\n",
    " \n",
    "\n",
    "而distributed representation中文释义是分布式表达，上面的表达方式可以改成：\n",
    "\n",
    "性别 老年 成年 婴儿\n",
    "\n",
    "[1,       0,      1,      0]表示成年男子\n",
    "\n",
    "[0,       0,      1,      0]表示成年女子\n",
    "\n",
    "[1,       1,      0,      0]表示老爷爷\n",
    "\n",
    "[0,       1,      0,      0]表示老奶奶\n",
    "\n",
    "[1,       0,      0,      1]表示男婴\n",
    "\n",
    "[0,       0,      0,      1]表示女婴\n",
    "\n",
    "如果我们想表达男童和女童，只需要增加一个特征维度即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding\n",
    "\n",
    "\n",
    "翻译成中文叫做词嵌入，这里的embedding来源于范畴论，在范畴论中称为morphism(态射)，态射表示两个数学结构中保持结构的一种过程抽象，比如“函数”、“映射”，他们都是表示一个域和另一个域之间的某种关系。\n",
    "\n",
    "范畴论中的嵌入(态射)是要保持结构的，而word embedding表示的是一种“降维”的嵌入，通过降维避免维度灾难，降低计算复杂度，从而更易于在深度学习中应用。\n",
    "\n",
    "理解了distributed representation和word embedding的概念，我们就初步了解了word2vec的本质，它其实是通过distributed representation的表达方式来表示词，而且通过降维的word embedding来减少计算量的一种方法\n",
    "\n",
    " \n",
    "\n",
    "## word2vec中的神经网络\n",
    "\n",
    "word2vec中做训练主要使用的是神经概率语言模型，这需要掌握一些基础知识，否则下面的内容比较难理解，关于神经网络训练词向量的基础知识我在《自己动手做聊天机器人 二十四-将深度学习应用到NLP》中有讲解，可以参考，这里不再赘述。\n",
    "\n",
    "在word2vec中使用的最重要的两个模型是CBOW和Skip-gram模型，下面我们分别来介绍这两种模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW模型\n",
    "\n",
    "CBOW全称是Continuous Bag-of-Words Model，是在已知当前词的上下文的前提下预测当前词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/6e168a6ea0fc1051ef2ae3192f7ce3b7626cd5e5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW模型的神经网络结构设计如下：\n",
    "\n",
    "输入层：词w的上下文一共2c个词的词向量\n",
    "\n",
    "投影层：将输入层的2c个向量做求和累加\n",
    "\n",
    "输出层：一个霍夫曼树，其中叶子节点是语料中出现过的词，权重是出现的次数\n",
    "\n",
    "我们发现这一设计相比《自己动手做聊天机器人 二十四-将深度学习应用到NLP》中讲到的神经网络模型把首尾相接改成了求和累加，这样减少了维度；去掉了隐藏层，这样减少了计算量；输出层由softmax归一化运算改成了霍夫曼树；这一系列修改对训练的性能有很大提升，而效果不减，这是独到之处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于霍夫曼树的Hierarchical Softmax技术\n",
    "\n",
    "上面的CBOW输出层为什么要建成一个霍夫曼树呢？因为我们是要基于训练语料得到每一个可能的w的概率。那么具体怎么得到呢？我们先来看一下这个霍夫曼树的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/ddb7360209957368d53c066deccb1c0d686feff4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个霍夫曼树中，我们以词足球为例，走过的路径图上容易看到，其中非根节点上的θ表示待训练的参数向量，也就是要达到这种效果：当在投射层产出了一个新的向量x，那么我通过逻辑回归公式：\n",
    "\n",
    "$σ(xTθ) = 1/(1+e^{(-x^Tθ)})$\n",
    "\n",
    "就可以得出在每一层被分到左节点(1)还是右节点(0)的概率分别是\n",
    "\n",
    "$p(d|x,θ) = 1-σ(xTθ)$\n",
    "\n",
    "和\n",
    "\n",
    "$p(d|x,θ) = σ(xTθ)$\n",
    "\n",
    "那么就有：\n",
    "\n",
    "$p(足球|Context(足球)) = ∏ p(d|x,θ)$\n",
    "\n",
    "现在模型已经有了，下面就是通过语料来训练v(Context(w))、x和θ的过程了\n",
    "\n",
    "我们以对数似然函数为优化目标，盗取一个网上的推导公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/e66ecb7bd782da7ee0512df278c30240ce753423.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设两个求和符号里面的部分记作L(w, j)，那么有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/6fd27ff26d8a54efd31d725d4659c10b6fadbdd3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是θ的更新公式：\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/91e37d7bac2f7f94577e62c2a0cc373c4f8f154d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理得出x的梯度公式：\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/58adbe7af6fe479fbffc7510a9ce10793c4a36fc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为x是多个v的累加，word2vec中v的更新方法是：\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/3f8446f41079621cfdcbf3e39d00cb31b154e784.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram模型\n",
    "\n",
    "Skip-gram全称是Continuous Skip-gram Model，是在已知当前词的情况下预测上下文\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/e4789eeb56746d55b6e710388bdd7f180c392da5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram模型的神经网络结构设计如下：\n",
    "\n",
    "输入层：w的词向量v(w)\n",
    "\n",
    "投影层：依然是v(w)，就是一个形式\n",
    "\n",
    "输出层：和CBOW一样的霍夫曼树\n",
    "\n",
    "后面的推导公式和CBOW大同小异，其中θ和v(w)的更新公式除了把符号名从x改成了v(w)之外完全一样，  \n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/8c5424a24c0f96566abb72d27808c75506aff841.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归神经网络 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聊天机器人是需要智能的，而如果他记不住任何信息，就谈不上智能，递归神经网络是一种可以存储记忆的神经网络，LSTM是递归神经网络的一种，在NLP领域应用效果不错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 递归神经网络  \n",
    "\n",
    "\n",
    "递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），另一种是结构递归神经网络（recursive neural network）。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。两者训练的算法不同，但属于同一算法变体（百度百科）。本节我们重点介绍时间递归神经网络，下面提到RNN特指时间递归神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间递归神经网络\n",
    "\n",
    "传统的神经网络叫做FNN(Feed-Forward Neural Networks)，也就是前向反馈神经网络，有关传统神经网络的介绍请见《机器学习教程 十二-神经网络模型的原理》，RNN是在此基础上引入了定向循环，也就是已神经元为节点组成的图中存在有向的环，这种神经网络可以表达某些前后关联关系，事实上，真正的生物神经元之间也是存在这种环形信息传播的，RNN也是神经网络向真实生物神经网络靠近的一个进步。一个典型的RNN是这样的：\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/5a3a6de28e660a08c09dc75bc16c38d88f850cd8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中隐藏层中的节点之间构成了全连接，也就是一个隐藏层节点的输出可以作为另一个隐藏层节点甚至它自己的输入\n",
    "\n",
    "这种结构可以抽象成：  \n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/489d09ae7d9c122a706ecded04e6a7bde406cf39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中U、V、W都是变换概率矩阵，x是输入，o是输出\n",
    "\n",
    "比较容易看出RNN的关键是隐藏层，因为隐藏层能够捕捉到序列的信息，也就是一种记忆的能力\n",
    "\n",
    "在RNN中U、V、W的参数都是共享的，也就是只需要关注每一步都在做相同的事情，只是输入不同，这样来降低参数个数和计算量\n",
    "\n",
    "RNN在NLP中的应用比较多，因为语言模型就是在已知已经出现的词的情况下预测下一个词的概率的，这正是一个有时序的模型，下一个词的出现取决于前几个词，刚好对应着RNN中隐藏层之间的内部连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN 训练方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习自动问答的一般方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聊天机器人本质上是一个范问答系统，既然是问答系统就离不开候选答案的选择，利用深度学习的方法可以帮助我们找到最佳的答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料库的获取方法\n",
    "\n",
    "对于一个范问答系统，一般我们从互联网上收集语料信息，比如百度、谷歌等，用这些结果构建问答对组成的语料库。然后把这些语料库分成多个部分：训练集、开发集、测试集\n",
    "\n",
    "问答系统训练其实是训练一个怎么在一堆答案里找到一个正确答案的模型，那么为了让样本更有效，在训练过程中我们不把所有答案都放到一个向量空间中，而是对他们做个分组，首先，我们在语料库里采集样本，收集每一个问题对应的500个答案集合，其中这500个里面有正向的样本，也会随机选一些负向样本放里面，这样就能突出这个正向样本的作用了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于CNN的系统设计\n",
    "\n",
    "CNN的三个优点：sparse interaction(稀疏的交互)，parameter sharing(参数共享)，equivalent respresentation(等价表示)。正是由于这三方面的优点，才更适合于自动问答系统中的答案选择模型的训练。\n",
    "\n",
    "我们设计卷积公式表示如下（不了解卷积的含义请见《机器学习教程 十五-细解卷积神经网络》）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/4dc33e729f50c04760fc0356971a7cf83bf2e1bd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设每个词用三维向量表示，左边是4个词，右边是卷积矩阵，那么得到输出为：\n",
    "\n",
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/8a8f50d1ca436d218a1374907745cfa13a4675ae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果基于这个结果做1-MaxPool池化，那么就取o中的最大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通用的训练方法\n",
    "\n",
    "训练时获取问题的词向量Vq(这里面词向量可以使用google的word2vec来训练,有关word2vec的内容可以看《自己动手做聊天机器人 二十五-google的文本挖掘深度学习工具word2vec的实现原理》)，和一个正向答案的词向量Va+，和一个负向答案的词向量Va-， 然后比较问题和这两个答案的相似度，两个相似度的差值如果大于一个阈值m就用来更新模型参数，然后继续在候选池里选答案，小于m就不更新模型，即优化函数为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/37318a24be94c53d84b748c00ffacf6a939bc84b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数更新方式和其他卷积神经网络方式相同，都是梯度下降、链式求导\n",
    "\n",
    "对于测试数据，计算问题和候选答案的cos距离，相似度最大的那个就是正确答案的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络结构设计\n",
    "\n",
    "以下是六种结构设计，解释一下，其中HL表示hide layer隐藏层，它的激活函数设计成z = tanh(Wx+B)，CNN是卷积层，P是池化层，池化步长为1，T是tanh层，P+T的输出是向量表示，最终的输出是两个向量的cos相似度\n",
    "\n",
    "图中HL或CNN连起来的表示他们共享相同的权重。CNN的输出是几维的取决于做多少个卷积特征，如果有4个卷积，那么结果就是4*3的矩阵(这里面的3在下一步被池化后就变成1维了)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/f476d12e8230f0d6d5de033e52d2dfee5015c18a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.shareditor.com/uploads/media/my-context/0001/01/92c731a4927a55c23ceefadcb7d823b694514298.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上结构的效果在论文《Applying Deep Learning To Answer Selection- A Study And An Open Task》中有详细说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "要把深度学习运用到聊天机器人中，关键在于以下几点：\n",
    "\n",
    "1. 对几种神经网络结构的选择、组合、优化\n",
    "\n",
    "2. 因为是有关自然语言处理，所以少不了能让机器识别的词向量\n",
    "\n",
    "3. 当涉及到相似或匹配关系时要考虑相似度计算，典型的方法是cos距离\n",
    "\n",
    "4. 如果需求涉及到文本序列的全局信息就用CNN或LSTM\n",
    "\n",
    "5. 当精度不高时可以加层\n",
    "\n",
    "6. 当计算量过大时别忘了参数共享和池化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "620px",
    "left": "0px",
    "right": "1115px",
    "top": "34px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
